---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# <span class="lang-switch" data-lang-en="ğŸ‘‹ About Me" data-lang-zh="ğŸ‘‹ ä¸ªäººç®€ä»‹">ğŸ‘‹ About Me</span>

<span class="lang-switch" data-lang-en="I am currently a PhD student jointly affiliated with the **State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS)** at the **Institute of Automation, Chinese Academy of Sciences (CASIA)** and **Zhongguancun Academy**, under the supervision of Prof. [Cheng-Lin Liu](https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN)." data-lang-zh="æœ¬äººç›®å‰æ˜¯ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿå›½å®¶é‡ç‚¹å®éªŒå®¤ï¼ˆMAISï¼‰ä¸ä¸­å…³æ‘å­¦é™¢è”åˆåŸ¹å…»çš„åšå£«ç ”ç©¶ç”Ÿï¼Œå¸ˆä»[åˆ˜æˆæ—](https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN)ç ”ç©¶å‘˜ã€‚">I am currently a PhD student jointly affiliated with the **State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS)** at the **Institute of Automation, Chinese Academy of Sciences (CASIA)** and **Zhongguancun Academy**, under the supervision of Prof. [Cheng-Lin Liu](https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN).</span>

## <span class="lang-switch" data-lang-en="ğŸ“ Education Background" data-lang-zh="ğŸ“ æ•™è‚²èƒŒæ™¯">ğŸ“ Education Background</span>
<div class="lang-switch" data-lang-en="<ul><li><strong>2024.09 - Present</strong>: Ph.D. in Pattern Recognition and Intelligent Systems, CASIA-MAIS</li><li><strong>2021.09 - 2024.06</strong>: M.S. in Electronic Information, NLPR, CASIA</li><li><strong>2017.09 - 2021.06</strong>: B.E. in Space Science and Technology, Xidian University</li></ul>" data-lang-zh="<ul><li><strong>2024.09 - è‡³ä»Š</strong>: æ¨¡å¼è¯†åˆ«ä¸æ™ºèƒ½ç³»ç»Ÿåšå£«ï¼Œä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€-MAIS</li><li><strong>2021.09 - 2024.06</strong>: ç”µå­ä¿¡æ¯ç¡•å£«ï¼Œä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€-NLPR</li><li><strong>2017.09 - 2021.06</strong>: ç©ºé—´ç§‘å­¦ä¸æŠ€æœ¯å­¦å£«ï¼Œè¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦</li></ul>">
- **2024.09 - Present**: Ph.D. in Pattern Recognition and Intelligent Systems, CASIA-MAIS
- **2021.09 - 2024.06**: M.S. in Electronic Information, NLPR, CASIA
- **2017.09 - 2021.06**: B.E. in Space Science and Technology, Xidian University
</div>

## <span class="lang-switch" data-lang-en="ğŸ”¬ Research Focus" data-lang-zh="ğŸ”¬ ç ”ç©¶æ–¹å‘">ğŸ”¬ Research Focus</span>
<span class="lang-switch" data-lang-en="My Ph.D. research centers on:" data-lang-zh="åšå£«ç ”ç©¶æ–¹å‘ä¸»è¦åŒ…æ‹¬ï¼š">My Ph.D. research centers on:</span>

<div class="lang-switch" data-lang-en="<ul><li><strong>ğŸ§¬ AI for Science</strong>: AI-driven vaccine adjuvant discovery and development</li><li><strong>ğŸ¤– Multimodal Large Language Models</strong>: Reliable reasoning, inference acceleration, and vision token optimization</li><li><strong>âœï¸ Handwritten Text Recognition & Generation</strong>: Online Chinese text recognition and synthesis</li></ul>" data-lang-zh="<ul><li><strong>ğŸ§¬ AI for Science</strong>: AIé©±åŠ¨çš„ç–«è‹—ä½å‰‚å‘ç°ä¸å¼€å‘</li><li><strong>ğŸ¤– å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹</strong>: å¯é æ¨ç†ã€æ¨ç†åŠ é€Ÿä¸è§†è§‰Tokenä¼˜åŒ–</li><li><strong>âœï¸ æ‰‹å†™æ–‡æœ¬è¯†åˆ«ä¸ç”Ÿæˆ</strong>: è”æœºä¸­æ–‡æ–‡æœ¬è¯†åˆ«ä¸åˆæˆ</li></ul>">
- **ğŸ§¬ AI for Science**: AI-driven vaccine adjuvant discovery and development
- **ğŸ¤– Multimodal Large Language Models**: Reliable reasoning, inference acceleration, and vision token optimization
- **âœï¸ Handwritten Text Recognition & Generation**: Online Chinese text recognition and synthesis
</div>

## <span class="lang-switch" data-lang-en="ğŸ“Š Academic Impact" data-lang-zh="ğŸ“Š å­¦æœ¯å½±å“">ğŸ“Š Academic Impact</span>
<span class="lang-switch" data-lang-en="You can find my publications on <a href='https://scholar.google.com/citations?user=XoiT9wMAAAAJ&hl=zh-CN'>Google Scholar</a> and connect with me through various academic platforms listed in the sidebar." data-lang-zh="æ‚¨å¯ä»¥åœ¨<a href='https://scholar.google.com/citations?user=XoiT9wMAAAAJ&hl=zh-CN'>Google Scholar</a>ä¸ŠæŸ¥çœ‹æˆ‘çš„è®ºæ–‡ï¼Œå¹¶é€šè¿‡ä¾§è¾¹æ åˆ—å‡ºçš„å„ç§å­¦æœ¯å¹³å°ä¸æˆ‘è”ç³»ã€‚">You can find my publications on <a href='https://scholar.google.com/citations?user=XoiT9wMAAAAJ&hl=zh-CN'>Google Scholar</a> and connect with me through various academic platforms listed in the sidebar.</span>


# <span class="lang-switch" data-lang-en="ğŸ”¥ News" data-lang-zh="ğŸ”¥ æœ€æ–°åŠ¨æ€">ğŸ”¥ News</span>
- *2026.02*: &nbsp;ğŸ‰ğŸ‰ **Three papers accepted to CVPR 2026**! Including "MeteorPred" (meteorological multimodal model), "ChartAgent" (chart understanding framework), and "Fine-Grained Post-Training Quantization" (LVLM optimization).
- *2026.01*: &nbsp;ğŸ‰ğŸ‰ **Three papers accepted to top-tier conferences**! Two papers to **ICLR 2026**: "An Open-Ended Benchmark for Adjuvant Research with MLLM" and "One Patch Doesn't Fit All" (adaptive patching for MLLMs). One paper to **ICRA 2026**: "RANGER" (monocular zero-shot semantic navigation).
- *2025.11*: &nbsp;ğŸ‰ğŸ‰ **One paper accepted to AAAI 2026**! "VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding" - a comprehensive framework for video anomaly detection and understanding. 

# <span class="lang-switch" data-lang-en="ğŸ“ Publications" data-lang-zh="ğŸ“ å­¦æœ¯è®ºæ–‡">ğŸ“ Publications</span> 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2026</div><img src='images/adj_bench.png' alt="adjuvant framework" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An Open-Ended Benchmark and Formal Framework for Adjuvant Research with MLLM](https://arxiv.org/)

**Yi Chen\***, Yu Zhang\*, Jian Xu, Xu-Yao Zhang, Hua Yue, Xinming Wang, Zequan Lyu, Wei Wei, Cheng-Lin Liu

**ICLR 2026** <strong><span class='show_paper_citations' data='XoiT9wMAAAAJ:PAPER_ID_1'></span></strong>
- First benchmark dedicated to adjuvant research using multimodal large language models
- Formal framework for representing adjuvant design principles and immune mechanisms
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='images/Recoverable Compression.png' alt="recoverable compression" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information](https://ojs.aaai.org/index.php/AAAI/article/view/32229)

**Yi Chen**, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu

**AAAI 2025** <strong><span class='show_paper_citations' data='XoiT9wMAAAAJ:PAPER_ID_2'></span></strong> | [**Code**](https://github.com/banjiuyufen/Recoverable-Compression)
- Text-guided dynamic visual token recovery mechanism for multimodal models
- Achieves comparable performance while compressing visual tokens to 10% of original quantity
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv</div><img src='images/VisTopo.png' alt="vistopo framework" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[VisTopo: Dynamic Spatial Topology Modeling for Fine-Grained Visual Prompting in Multimodal Reasoning](https://arxiv.org/abs/2409.01162)

**Yi Chen**, MingMing Yu, Jie Gu, Chu Tang, Jingmin Chen, Rui-Qi Wang

**arXiv 2024** <strong><span class='show_paper_citations' data='XoiT9wMAAAAJ:PAPER_ID_3'></span></strong>
- Dual-stream prompting mechanism for modeling visual scene structure
- Achieves state-of-the-art 97.6% precision in mitigating visual hallucination
</div>
</div>

## <span class="lang-switch" data-lang-en="ğŸ¤– AI for Science & Scientific Computing" data-lang-zh="ğŸ¤– AI for Science ä¸ç§‘å­¦è®¡ç®—">ğŸ¤– AI for Science & Scientific Computing</span>

- [An Open-Ended Benchmark and Formal Framework for Adjuvant Research with MLLM](https://arxiv.org/), **Yi Chen\***, Yu Zhang\*, Jian Xu, et al. **ICLR 2026**

- [An Efficient Strategy for Data-constrained Machine Learning in Materials Science](https://arxiv.org/), ChunTing Shao\*, **Yi Chen\***, ShanMan Song, et al. **arXiv 2024**

- [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859), Shuo Tang, Jian Xu, Jiadong Zhang, **Yi Chen**, et al. **CVPR 2026**

## <span class="lang-switch" data-lang-en="ğŸ” Multimodal Large Language Models" data-lang-zh="ğŸ” å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹">ğŸ” Multimodal Large Language Models</span>

- [Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information](https://ojs.aaai.org/index.php/AAAI/article/view/32229), **Yi Chen**, Jian Xu, Xu-Yao Zhang, et al. **AAAI 2025** | [**Code**](https://github.com/banjiuyufen/Recoverable-Compression)

- [VisTopo: Dynamic Spatial Topology Modeling for Fine-Grained Visual Prompting in Multimodal Reasoning](https://arxiv.org/abs/2409.01162), **Yi Chen**, MingMing Yu, Jie Gu, et al. **arXiv 2024**

- [Sparsity Meets Similarity: Leveraging Long-Tail Distribution for Dynamic Optimized Token Representation in Multimodal Large Language Models](https://arxiv.org/abs/2409.01162), **Yi Chen\***, Gao-Tong Yu\*, Jian Xu. **arXiv 2024**

- [One Patch Doesn't Fit All: Adaptive Patching for Native-Resolution Multimodal Large Language Models](https://arxiv.org/), Wenzhuo Liu, Weijie Yin, Fei Zhu, Shijie Ma, Haiyang Guo, **Yi Chen**, et al. **ICLR 2026**

- [Fine-Grained Post-Training Quantization for Large Vision Language Models with Integrated Gradients](https://arxiv.org/abs/2507.21649), Ziwen Xiang, Fanhu Zeng, Hongjian Fang, Rui-Qi Wang, Renxing Chen, **Yi Chen**, et al. **CVPR 2026**

## <span class="lang-switch" data-lang-en="ğŸ§  Machine Learning & Few-Shot Learning" data-lang-zh="ğŸ§  æœºå™¨å­¦ä¹ ä¸å°æ ·æœ¬å­¦ä¹ ">ğŸ§  Machine Learning & Few-Shot Learning</span>

- [ManiNet: Manifold Network for Few-Shot Learning](https://arxiv.org/), Ruiqi Wang, Hengcan Shi, **Yi Chen**, YaoNan Wang. **AIHCIR 2025** (**Best Paper Award**)

## <span class="lang-switch" data-lang-en="ğŸ¯ Computer Vision & Robotics" data-lang-zh="ğŸ¯ è®¡ç®—æœºè§†è§‰ä¸æœºå™¨äºº">ğŸ¯ Computer Vision & Robotics</span>

- [RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation](https://arxiv.org/abs/2512.24212), Ming-Ming Yu, **Yi Chen**, BÃ¶rje F. Karlsson, Wenjun Wu. **ICRA 2026**

- [ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning](https://arxiv.org/abs/2512.14040), Boran Wang, Xinming Wang, **Yi Chen**, et al. **CVPR Finding 2026**

- [VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding](https://arxiv.org/abs/2507.21507), Shibo Gao, Peipei Yang, **Yi Chen**, et al. **AAAI 2026**

- [The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM](https://arxiv.org/abs/2507.21649), Shibo Gao, Peipei Yang, Haiyang Guo, Yangyang Liu, **Yi Chen**, et al. **arXiv 2024**

## <span class="lang-switch" data-lang-en="ğŸ“Š NLP & Information Processing" data-lang-zh="ğŸ“Š è‡ªç„¶è¯­è¨€å¤„ç†ä¸ä¿¡æ¯å¤„ç†">ğŸ“Š NLP & Information Processing</span>

- [ElementCheck: Long-Form Text Factuality Evaluation via Sentence-Level Fact Elements](https://arxiv.org/abs/2512.14040), Xinming Wang, Haoran Du, **Yi Chen**, et al. **arXiv 2024**

- [MR-ALIGN: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models](https://arxiv.org/abs/2510.24794), Xinming Wang, Jian Xu, Bin Yu, Sheng Lian, Hongzhu Yi, **Yi Chen**, et al. **arXiv 2024**

- [The Hitchhiker's Guide to Scientific Agents: A Journey Through the Cosmos of Research Automation](https://www.techrxiv.org/users/951553/articles/1320864-the-hitchhiker-s-guide-to-autonomous-research-a-survey-of-scientific-agents), Xinming Wang, Aslan Feng, Jian Xu, **Yi Chen**, et al. **TechRxiv 2024** | [**GitHub**](https://github.com/gudehhh666/Awesome_Scientific_Agent)

## <span class="lang-switch" data-lang-en="âœï¸ Handwritten Text Recognition & Generation" data-lang-zh="âœï¸ æ‰‹å†™æ–‡æœ¬è¯†åˆ«ä¸ç”Ÿæˆ">âœï¸ Handwritten Text Recognition & Generation</span>

- [Recognition of Online Handwritten Chinese Texts in Any Writing Direction via Stroke Classification Based Over-Segmentation](https://link.springer.com/chapter/10.1007/978-3-031-78183-4_24), **Yi Chen**, Heng Zhang, Min-Si Ren, Cheng-Lin Liu. **ICPR 2024**

- [Improved Learning for Online Handwritten Chinese Text Recognition with Convolutional Prototype Network](https://link.springer.com/chapter/10.1007/978-3-031-41685-9_3), **Yi Chen**, Heng Zhang, Cheng-Lin Liu. **ICDAR 2023**

- [Context-Aware Confidence Estimation for Rejection in Handwritten Chinese Text Recognition](https://link.springer.com/chapter/10.1007/978-3-031-70533-5_9), Yang-Yang Liu, **Yi Chen**, Fei Yin, Cheng-Lin Liu. **ICDAR 2024**

- [Decoupling Layout from Glyph in Online Chinese Handwriting Generation](https://arxiv.org/abs/2410.02309), Min-Si Ren, Yan-Ming Zhang, **Yi Chen**. **ICLR 2025** | [**Code**](https://github.com/singularityrms/OLHWG)

# <span class="lang-switch" data-lang-en="ğŸ– Honors and Awards" data-lang-zh="ğŸ– è£èª‰å¥–é¡¹">ğŸ– Honors and Awards</span>

<div class="lang-switch" data-lang-en="<ul><li><em>2025</em> <strong>Academic Research Star</strong>, National AI Academy Beijing Zhongguancun Academy</li><li><em>2025</em> <strong>Best Paper Award</strong>, AIHCIR 2025 (for ManiNet: Manifold Network for Few-Shot Learning)</li><li><em>2024</em> <strong>3rd Place</strong>, ICDAR2024 Competition on Multi Font Group Recognition and OCR</li></ul>" data-lang-zh="<ul><li><em>2025</em> <strong>å­¦æœ¯ç§‘ç ”ä¹‹æ˜Ÿ</strong>ï¼Œå›½å®¶äººå·¥æ™ºèƒ½å­¦é™¢åŒ—äº¬ä¸­å…³æ‘å­¦é™¢</li><li><em>2025</em> <strong>æœ€ä½³è®ºæ–‡å¥–</strong>ï¼ŒAIHCIR 2025ï¼ˆManiNet: Manifold Network for Few-Shot Learningï¼‰</li><li><em>2024</em> <strong>ç¬¬ä¸‰å</strong>ï¼ŒICDAR2024å¤šå­—ä½“ç»„è¯†åˆ«ä¸OCRç«èµ›</li></ul>">
- *2025* **Academic Research Star**, National AI Academy Beijing Zhongguancun Academy
- *2025* **Best Paper Award**, AIHCIR 2025 (for "ManiNet: Manifold Network for Few-Shot Learning")
- *2024* **3rd Place**, ICDAR2024 Competition on Multi Font Group Recognition and OCR
</div> 

# <span class="lang-switch" data-lang-en="ğŸ“– Education" data-lang-zh="ğŸ“– æ•™è‚²ç»å†">ğŸ“– Education</span>
- *2024.09 - Present*, **Ph.D. in Pattern Recognition and Intelligent Systems**  
  State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences & Zhongguancun Academy  
  Supervisor: Prof. [Cheng-Lin Liu](https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN)

- *2021.09 - 2024.06*, **M.S. in Electronic Information**  
  National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences  
  Supervisor: Prof. [Cheng-Lin Liu](https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN)

- *2017.09 - 2021.06*, **B.E. in Space Science and Technology**  
  School of Space Science and Technology, Xidian University 

# <span class="lang-switch" data-lang-en="ğŸ”¬ Research Interests" data-lang-zh="ğŸ”¬ ç ”ç©¶å…´è¶£">ğŸ”¬ Research Interests</span>
- **AI for Science**: Applying artificial intelligence to scientific discovery, particularly in adjuvant research and materials science
- **Multimodal Large Language Models (MLLMs)**: Developing robust and efficient multimodal AI systems
- **Online Handwritten Text Recognition**: Recognition and generation of handwritten Chinese text
- **Computer Vision**: Image understanding, visual reasoning, and multimodal perception

# <span class="lang-switch" data-lang-en="ğŸ¤ Academic Service" data-lang-zh="ğŸ¤ å­¦æœ¯æœåŠ¡">ğŸ¤ Academic Service</span>
- **Program Committee Member**: AAAI 2026
- **Reviewer**: ICLR 2026, CVPR 2026, ICML 2026, ECCV 2026

# <span class="lang-switch" data-lang-en="ğŸŒŸ Open Source Contributions" data-lang-zh="ğŸŒŸ å¼€æºè´¡çŒ®">ğŸŒŸ Open Source Contributions</span>
- **PaddleScience Contributor**: Integrated Crystal Graph CNN (CGCNN) model for materials chemistry applications | [PR #977](https://github.com/PaddlePaddle/PaddleScience/pull/977)
  - Implemented full pipeline for crystal structure data processing and graph neural network training
  - Code merged into official repository and featured as an official case study

# <span class="lang-switch" data-lang-en="ğŸ“§ Contact" data-lang-zh="ğŸ“§ è”ç³»æ–¹å¼">ğŸ“§ Contact</span>
- **Email**: yi.chen@nlpr.ia.ac.cn
- **Office**: State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences
- **Address**: Beijing 100190, China

---

*Open to collaboration and academic exchange. Please feel free to contact me via email.*