---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# <span class="lang-switch" data-lang-en="üëã About Me" data-lang-zh="üëã ‰∏™‰∫∫ÁÆÄ‰ªã">üëã About Me</span>

<span class="lang-switch" data-lang-en="I am currently a PhD student jointly affiliated with the **State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS)** at the **Institute of Automation, Chinese Academy of Sciences (CASIA)** and **Zhongguancun Academy**, under the supervision of Prof. [Cheng-Lin Liu](https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN)." data-lang-zh="Êú¨‰∫∫ÁõÆÂâçÊòØ‰∏≠ÂõΩÁßëÂ≠¶Èô¢Ëá™Âä®ÂåñÁ†îÁ©∂ÊâÄÂ§öÊ®°ÊÄÅ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÂõΩÂÆ∂ÈáçÁÇπÂÆûÈ™åÂÆ§ÔºàMAISÔºâ‰∏é‰∏≠ÂÖ≥ÊùëÂ≠¶Èô¢ËÅîÂêàÂüπÂÖªÁöÑÂçöÂ£´Á†îÁ©∂ÁîüÔºåÂ∏à‰ªé[ÂàòÊàêÊûó](https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN)Á†îÁ©∂Âëò„ÄÇ">I am currently a PhD student jointly affiliated with the **State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS)** at the **Institute of Automation, Chinese Academy of Sciences (CASIA)** and **Zhongguancun Academy**, under the supervision of Prof. [Cheng-Lin Liu](https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN).</span>

## <span class="lang-switch" data-lang-en="üéì Education Background" data-lang-zh="üéì ÊïôËÇ≤ËÉåÊôØ">üéì Education Background</span>
<div class="lang-switch" data-lang-en="
- **2024.09 - Present**: Ph.D. in Pattern Recognition and Intelligent Systems, CASIA-MAIS
- **2021.09 - 2024.06**: M.S. in Electronic Information, NLPR, CASIA
- **2017.09 - 2021.06**: B.E. in Space Science and Technology, Xidian University
" data-lang-zh="
- **2024.09 - Ëá≥‰ªä**: Ê®°ÂºèËØÜÂà´‰∏éÊô∫ËÉΩÁ≥ªÁªüÂçöÂ£´Ôºå‰∏≠ÁßëÈô¢Ëá™Âä®ÂåñÊâÄ-MAIS
- **2021.09 - 2024.06**: ÁîµÂ≠ê‰ø°ÊÅØÁ°ïÂ£´Ôºå‰∏≠ÁßëÈô¢Ëá™Âä®ÂåñÊâÄ-NLPR
- **2017.09 - 2021.06**: Á©∫Èó¥ÁßëÂ≠¶‰∏éÊäÄÊúØÂ≠¶Â£´ÔºåË•øÂÆâÁîµÂ≠êÁßëÊäÄÂ§ßÂ≠¶
">
- **2024.09 - Present**: Ph.D. in Pattern Recognition and Intelligent Systems, CASIA-MAIS
- **2021.09 - 2024.06**: M.S. in Electronic Information, NLPR, CASIA
- **2017.09 - 2021.06**: B.E. in Space Science and Technology, Xidian University
</div>

## <span class="lang-switch" data-lang-en="üî¨ Research Focus" data-lang-zh="üî¨ Á†îÁ©∂ÊñπÂêë">üî¨ Research Focus</span>
<span class="lang-switch" data-lang-en="My Ph.D. research centers on:" data-lang-zh="ÂçöÂ£´Á†îÁ©∂ÊñπÂêë‰∏ªË¶ÅÂåÖÊã¨Ôºö">My Ph.D. research centers on:</span>

<div class="lang-switch" data-lang-en="
- **üß¨ AI for Science**: AI-driven vaccine adjuvant discovery and development
- **ü§ñ Multimodal Large Language Models**: Reliable reasoning, inference acceleration, and vision token optimization
- **‚úçÔ∏è Handwritten Text Recognition & Generation**: Online Chinese text recognition and synthesis
" data-lang-zh="
- **üß¨ AI for Science**: AIÈ©±Âä®ÁöÑÁñ´Ëãó‰ΩêÂâÇÂèëÁé∞‰∏éÂºÄÂèë
- **ü§ñ Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã**: ÂèØÈù†Êé®ÁêÜ„ÄÅÊé®ÁêÜÂä†ÈÄü‰∏éËßÜËßâToken‰ºòÂåñ
- **‚úçÔ∏è ÊâãÂÜôÊñáÊú¨ËØÜÂà´‰∏éÁîüÊàê**: ËÅîÊú∫‰∏≠ÊñáÊñáÊú¨ËØÜÂà´‰∏éÂêàÊàê
">
- **üß¨ AI for Science**: AI-driven vaccine adjuvant discovery and development
- **ü§ñ Multimodal Large Language Models**: Reliable reasoning, inference acceleration, and vision token optimization
- **‚úçÔ∏è Handwritten Text Recognition & Generation**: Online Chinese text recognition and synthesis
</div>

## <span class="lang-switch" data-lang-en="üìä Academic Impact" data-lang-zh="üìä Â≠¶ÊúØÂΩ±Âìç">üìä Academic Impact</span>
<span class="lang-switch" data-lang-en="You can find my publications on <a href='https://scholar.google.com/citations?user=XoiT9wMAAAAJ&hl=zh-CN'>Google Scholar</a> and connect with me through various academic platforms listed in the sidebar." data-lang-zh="ÊÇ®ÂèØ‰ª•Âú®<a href='https://scholar.google.com/citations?user=XoiT9wMAAAAJ&hl=zh-CN'>Google Scholar</a>‰∏äÊü•ÁúãÊàëÁöÑËÆ∫ÊñáÔºåÂπ∂ÈÄöËøá‰æßËæπÊ†èÂàóÂá∫ÁöÑÂêÑÁßçÂ≠¶ÊúØÂπ≥Âè∞‰∏éÊàëËÅîÁ≥ª„ÄÇ">You can find my publications on <a href='https://scholar.google.com/citations?user=XoiT9wMAAAAJ&hl=zh-CN'>Google Scholar</a> and connect with me through various academic platforms listed in the sidebar.</span>


# <span class="lang-switch" data-lang-en="üî• News" data-lang-zh="üî• ÊúÄÊñ∞Âä®ÊÄÅ">üî• News</span>
- *2026.02*: &nbsp;üéâüéâ **Three papers accepted to CVPR 2026**! Including "MeteorPred" (meteorological multimodal model), "ChartAgent" (chart understanding framework), and "Fine-Grained Post-Training Quantization" (LVLM optimization).
- *2026.01*: &nbsp;üéâüéâ **Three papers accepted to top-tier conferences**! Two papers to **ICLR 2026**: "An Open-Ended Benchmark for Adjuvant Research with MLLM" and "One Patch Doesn't Fit All" (adaptive patching for MLLMs). One paper to **ICRA 2026**: "RANGER" (monocular zero-shot semantic navigation).
- *2025.11*: &nbsp;üéâüéâ **One paper accepted to AAAI 2026**! "VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding" - a comprehensive framework for video anomaly detection and understanding. 

# <span class="lang-switch" data-lang-en="üìù Publications" data-lang-zh="üìù Â≠¶ÊúØËÆ∫Êñá">üìù Publications</span> 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2026</div><img src='images/adj_bench.png' alt="adjuvant framework" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An Open-Ended Benchmark and Formal Framework for Adjuvant Research with MLLM](https://arxiv.org/)

**Yi Chen\***, Yu Zhang\*, Jian Xu, Xu-Yao Zhang, Hua Yue, Xinming Wang, Zequan Lyu, Wei Wei, Cheng-Lin Liu

**ICLR 2026** <strong><span class='show_paper_citations' data='XoiT9wMAAAAJ:PAPER_ID_1'></span></strong>
- First benchmark dedicated to adjuvant research using multimodal large language models
- Formal framework for representing adjuvant design principles and immune mechanisms
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='images/Recoverable Compression.png' alt="recoverable compression" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information](https://ojs.aaai.org/index.php/AAAI/article/view/32229)

**Yi Chen**, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu

**AAAI 2025** <strong><span class='show_paper_citations' data='XoiT9wMAAAAJ:PAPER_ID_2'></span></strong> | [**Code**](https://github.com/banjiuyufen/Recoverable-Compression)
- Text-guided dynamic visual token recovery mechanism for multimodal models
- Achieves comparable performance while compressing visual tokens to 10% of original quantity
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv</div><img src='images/VisTopo.png' alt="vistopo framework" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[VisTopo: Dynamic Spatial Topology Modeling for Fine-Grained Visual Prompting in Multimodal Reasoning](https://arxiv.org/abs/2409.01162)

**Yi Chen**, MingMing Yu, Jie Gu, Chu Tang, Jingmin Chen, Rui-Qi Wang

**arXiv 2024** <strong><span class='show_paper_citations' data='XoiT9wMAAAAJ:PAPER_ID_3'></span></strong>
- Dual-stream prompting mechanism for modeling visual scene structure
- Achieves state-of-the-art 97.6% precision in mitigating visual hallucination
</div>
</div>

## <span class="lang-switch" data-lang-en="ü§ñ AI for Science & Scientific Computing" data-lang-zh="ü§ñ AI for Science ‰∏éÁßëÂ≠¶ËÆ°ÁÆó">ü§ñ AI for Science & Scientific Computing</span>

- [An Open-Ended Benchmark and Formal Framework for Adjuvant Research with MLLM](https://arxiv.org/), **Yi Chen\***, Yu Zhang\*, Jian Xu, et al. **ICLR 2026**

- [An Efficient Strategy for Data-constrained Machine Learning in Materials Science](https://arxiv.org/), ChunTing Shao\*, **Yi Chen\***, ShanMan Song, et al. **arXiv 2024**

- [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859), Shuo Tang, Jian Xu, Jiadong Zhang, **Yi Chen**, et al. **CVPR 2026**

## <span class="lang-switch" data-lang-en="üîç Multimodal Large Language Models" data-lang-zh="üîç Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã">üîç Multimodal Large Language Models</span>

- [Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information](https://ojs.aaai.org/index.php/AAAI/article/view/32229), **Yi Chen**, Jian Xu, Xu-Yao Zhang, et al. **AAAI 2025** | [**Code**](https://github.com/banjiuyufen/Recoverable-Compression)

- [VisTopo: Dynamic Spatial Topology Modeling for Fine-Grained Visual Prompting in Multimodal Reasoning](https://arxiv.org/abs/2409.01162), **Yi Chen**, MingMing Yu, Jie Gu, et al. **arXiv 2024**

- [Sparsity Meets Similarity: Leveraging Long-Tail Distribution for Dynamic Optimized Token Representation in Multimodal Large Language Models](https://arxiv.org/abs/2409.01162), **Yi Chen\***, Gao-Tong Yu\*, Jian Xu. **arXiv 2024**

- [One Patch Doesn't Fit All: Adaptive Patching for Native-Resolution Multimodal Large Language Models](https://arxiv.org/), Wenzhuo Liu, Weijie Yin, Fei Zhu, Shijie Ma, Haiyang Guo, **Yi Chen**, et al. **ICLR 2026**

- [Fine-Grained Post-Training Quantization for Large Vision Language Models with Integrated Gradients](https://arxiv.org/abs/2507.21649), Ziwen Xiang, Fanhu Zeng, Hongjian Fang, Rui-Qi Wang, Renxing Chen, **Yi Chen**, et al. **CVPR 2026**

## <span class="lang-switch" data-lang-en="üß† Machine Learning & Few-Shot Learning" data-lang-zh="üß† Êú∫Âô®Â≠¶‰π†‰∏éÂ∞èÊ†∑Êú¨Â≠¶‰π†">üß† Machine Learning & Few-Shot Learning</span>

- [ManiNet: Manifold Network for Few-Shot Learning](https://arxiv.org/), Ruiqi Wang, Hengcan Shi, **Yi Chen**, YaoNan Wang. **AIHCIR 2025** (**Best Paper Award**)

## <span class="lang-switch" data-lang-en="üéØ Computer Vision & Robotics" data-lang-zh="üéØ ËÆ°ÁÆóÊú∫ËßÜËßâ‰∏éÊú∫Âô®‰∫∫">üéØ Computer Vision & Robotics</span>

- [RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation](https://arxiv.org/abs/2512.24212), Ming-Ming Yu, **Yi Chen**, B√∂rje F. Karlsson, Wenjun Wu. **ICRA 2026**

- [ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning](https://arxiv.org/abs/2512.14040), Boran Wang, Xinming Wang, **Yi Chen**, et al. **CVPR Finding 2026**

- [VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding](https://arxiv.org/abs/2507.21507), Shibo Gao, Peipei Yang, **Yi Chen**, et al. **AAAI 2026**

- [The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM](https://arxiv.org/abs/2507.21649), Shibo Gao, Peipei Yang, Haiyang Guo, Yangyang Liu, **Yi Chen**, et al. **arXiv 2024**

## <span class="lang-switch" data-lang-en="üìä NLP & Information Processing" data-lang-zh="üìä Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏é‰ø°ÊÅØÂ§ÑÁêÜ">üìä NLP & Information Processing</span>

- [ElementCheck: Long-Form Text Factuality Evaluation via Sentence-Level Fact Elements](https://arxiv.org/abs/2512.14040), Xinming Wang, Haoran Du, **Yi Chen**, et al. **arXiv 2024**

- [MR-ALIGN: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models](https://arxiv.org/abs/2510.24794), Xinming Wang, Jian Xu, Bin Yu, Sheng Lian, Hongzhu Yi, **Yi Chen**, et al. **arXiv 2024**

- [The Hitchhiker's Guide to Scientific Agents: A Journey Through the Cosmos of Research Automation](https://www.techrxiv.org/users/951553/articles/1320864-the-hitchhiker-s-guide-to-autonomous-research-a-survey-of-scientific-agents), Xinming Wang, Aslan Feng, Jian Xu, **Yi Chen**, et al. **TechRxiv 2024** | [**GitHub**](https://github.com/gudehhh666/Awesome_Scientific_Agent)

## <span class="lang-switch" data-lang-en="‚úçÔ∏è Handwritten Text Recognition & Generation" data-lang-zh="‚úçÔ∏è ÊâãÂÜôÊñáÊú¨ËØÜÂà´‰∏éÁîüÊàê">‚úçÔ∏è Handwritten Text Recognition & Generation</span>

- [Recognition of Online Handwritten Chinese Texts in Any Writing Direction via Stroke Classification Based Over-Segmentation](https://link.springer.com/chapter/10.1007/978-3-031-78183-4_24), **Yi Chen**, Heng Zhang, Min-Si Ren, Cheng-Lin Liu. **ICPR 2024**

- [Improved Learning for Online Handwritten Chinese Text Recognition with Convolutional Prototype Network](https://link.springer.com/chapter/10.1007/978-3-031-41685-9_3), **Yi Chen**, Heng Zhang, Cheng-Lin Liu. **ICDAR 2023**

- [Context-Aware Confidence Estimation for Rejection in Handwritten Chinese Text Recognition](https://link.springer.com/chapter/10.1007/978-3-031-70533-5_9), Yang-Yang Liu, **Yi Chen**, Fei Yin, Cheng-Lin Liu. **ICDAR 2024**

- [Decoupling Layout from Glyph in Online Chinese Handwriting Generation](https://arxiv.org/abs/2410.02309), Min-Si Ren, Yan-Ming Zhang, **Yi Chen**. **ICLR 2025** | [**Code**](https://github.com/singularityrms/OLHWG)

# <span class="lang-switch" data-lang-en="üéñ Honors and Awards" data-lang-zh="üéñ Ëç£Ë™âÂ•ñÈ°π">üéñ Honors and Awards</span>

<div class="lang-switch" data-lang-en="
- *2025* **Academic Research Star**, National AI Academy Beijing Zhongguancun Academy
- *2025* **Best Paper Award**, AIHCIR 2025 (for 'ManiNet: Manifold Network for Few-Shot Learning')
- *2024* **3rd Place**, ICDAR2024 Competition on Multi Font Group Recognition and OCR
" data-lang-zh="
- *2025* **Â≠¶ÊúØÁßëÁ†î‰πãÊòü**ÔºåÂõΩÂÆ∂‰∫∫Â∑•Êô∫ËÉΩÂ≠¶Èô¢Âåó‰∫¨‰∏≠ÂÖ≥ÊùëÂ≠¶Èô¢
- *2025* **ÊúÄ‰Ω≥ËÆ∫ÊñáÂ•ñ**ÔºåAIHCIR 2025ÔºàManiNet: Manifold Network for Few-Shot LearningÔºâ
- *2024* **Á¨¨‰∏âÂêç**ÔºåICDAR2024Â§öÂ≠ó‰ΩìÁªÑËØÜÂà´‰∏éOCRÁ´ûËµõ
">
- *2025* **Academic Research Star**, National AI Academy Beijing Zhongguancun Academy
- *2025* **Best Paper Award**, AIHCIR 2025 (for "ManiNet: Manifold Network for Few-Shot Learning")
- *2024* **3rd Place**, ICDAR2024 Competition on Multi Font Group Recognition and OCR
</div> 

# <span class="lang-switch" data-lang-en="üìñ Education" data-lang-zh="üìñ ÊïôËÇ≤ÁªèÂéÜ">üìñ Education</span>
- *2024.09 - Present*, **Ph.D. in Pattern Recognition and Intelligent Systems**  
  State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences & Zhongguancun Academy  
  Supervisor: Prof. [Cheng-Lin Liu](https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN)

- *2021.09 - 2024.06*, **M.S. in Electronic Information**  
  National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences  
  Supervisor: Prof. [Cheng-Lin Liu](https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=zh-CN)

- *2017.09 - 2021.06*, **B.E. in Space Science and Technology**  
  School of Space Science and Technology, Xidian University 

# <span class="lang-switch" data-lang-en="üî¨ Research Interests" data-lang-zh="üî¨ Á†îÁ©∂ÂÖ¥Ë∂£">üî¨ Research Interests</span>
- **AI for Science**: Applying artificial intelligence to scientific discovery, particularly in adjuvant research and materials science
- **Multimodal Large Language Models (MLLMs)**: Developing robust and efficient multimodal AI systems
- **Online Handwritten Text Recognition**: Recognition and generation of handwritten Chinese text
- **Computer Vision**: Image understanding, visual reasoning, and multimodal perception

# <span class="lang-switch" data-lang-en="ü§ù Academic Service" data-lang-zh="ü§ù Â≠¶ÊúØÊúçÂä°">ü§ù Academic Service</span>
- **Program Committee Member**: AAAI 2026
- **Reviewer**: ICLR 2026, CVPR 2026, ICML 2026, ECCV 2026

# <span class="lang-switch" data-lang-en="üåü Open Source Contributions" data-lang-zh="üåü ÂºÄÊ∫êË¥°ÁåÆ">üåü Open Source Contributions</span>
- **PaddleScience Contributor**: Integrated Crystal Graph CNN (CGCNN) model for materials chemistry applications | [PR #977](https://github.com/PaddlePaddle/PaddleScience/pull/977)
  - Implemented full pipeline for crystal structure data processing and graph neural network training
  - Code merged into official repository and featured as an official case study

# <span class="lang-switch" data-lang-en="üìß Contact" data-lang-zh="üìß ËÅîÁ≥ªÊñπÂºè">üìß Contact</span>
- **Email**: yi.chen@nlpr.ia.ac.cn
- **Office**: State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences
- **Address**: Beijing 100190, China

---

*Open to collaboration and academic exchange. Please feel free to contact me via email.*